---
layout: post
title: What is Apache Spark?
feature-img: "assets/img/sample_feature_img.png"
tags: [Big Data]

---
**An introduction to Spark and its role in Big Data** 

When it comes to big data, processing speed is a big factor when choosing which software to use. Imagine trying to query hundreds of terabytes of data in MySQL. It could take hours for a single query to run! One of the reasons is because MySQL only uses one CPU core but Spark uses all cores and uses parallel computing to run queries. In fact, queries on Spark tend to process 10 times faster than on MySQL! Of course MySQL and Spark have very different use cases (MySQL is a database and Spark is a data processing engine). Spark can also split up a query into smaller queries and run those across different servers. If a company manages terabytes of data a day, they may want a way to run analytics or apply a machine learning algorithm on the data. Big data software such as Spark help achieve this. 

For those of you who are familiar with big data, you may be wondering how Hadoop plays a role in all of this. Before Spark was introduced in around 2015-2016, Hadoop was used in big data work. Hadoop aims to accomplish what Spark does but most engineers agree that Spark is more advanced, faster, and is now more widespread in the industry. One disadvantage of Spark however is that it does not have a native file system (unlike Hadoop). Nonetheless, in a time where industries demand real-time data to be used in marketing or machine learning, Spark is dominating.

| Phrase             | Definition                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
|--------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Dataframe          | A data structure which contains a distributed collection of data organized in columns. You can picture this as a relational database except dataframes are usually distributed among different servers/nodes in order to maximize speed. The great part about dataframes is that all the distributed data is abstracted away from the user.                                                                                                                                                                                                                                                                                                                                                                               |
| Spark SQL          | Allows users to run SQL queries on Dataframes                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| Driver/Master Node | The running the main function of the application which runs on the master node of the machine. In the meantime, it also declares transformations and actions on data RDDs.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Worker/Slave Node  | Any node which runs the program is called a slave node. Workers nodes each have executors which run an actual task. Once the worker node finishes running a task, it sends the result back to the driver.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| Spark Context      | Main entry point for any Spark functionality. You must import and initialize a spark context before you can use any spark functionality.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| Spark Streaming    | Spark supports streaming data in order to provide live analytics and near real time insights to data. There are two different ways Spark allows streaming: Spark Streaming and Structured Streaming. Spark streaming uses DStreams which means each chunk of data that's gets processed is in the format of an RDD. Generally programmers are moving away from Spark streaming because it is difficult to use and the APIs are not very consistent to the RDD ones. Structured streaming is the recommended way to stream data with Spark since it is based off Dataframes. It is generally much easier to use, guarantees that data is processed once, and allows us to use Dataframe API methods (such as SQL queries). |
                                                                                                                                                                                                                                                                                                                                                                                               
